<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="CodeV-R1: Reasoning-Enhanced Verilog Generation">
  <meta name="keywords" content="Language Language Model, Program synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CodeV-R1: Reasoning-Enhanced Verilog Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- highlight code -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a>Yaoyu Zhu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Di Huang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Hanqi Lyu</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a>Xiaoyun Zhang</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Chongxiao Li</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Wenxuan Shi</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Yutong Wu</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Jianan Mu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Jinghua Wang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Yang Zhao</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Pengwei Jin</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Shuyao Cheng</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Shengwen Liang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Xishan Zhang</a><sup>1,4</sup>,
              </span>
              <span class="author-block">
                <a>Rui Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Zidong Du</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Qi Guo</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Xing Hu</a><sup>1</sup><span class="symbol">✉</span>,
              </span>
              <span class="author-block">
                <a>Yunji Chen</a><sup>1,3</sup><span class="symbol">✉</span>
              </span>
            </div>
          
            <div class="affiliations">
              <span class="author-block"><sup>1</sup>SKL of Processors, Institute of Computing Technology, CAS</span><br>
              <span class="author-block"><sup>2</sup>University of Science and Technology of China</span><br>
              <span class="author-block"><sup>3</sup>University of Chinese Academy of Sciences</span><br>
              <span class="author-block"><sup>4</sup>Cambricon Technologies</span>
            </div>

            <!-- <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. ->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2407.10424" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. ->
                <span class="link-block">
                  <a href="https://github.com/iprc-dip/CodeV" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Model Link. ->
                <span class="link-block">
                  <a href="https://huggingface.co/yang-z"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-cloud-download-alt"></i>
                    </span>
                    <span>Download</span>
                  </a>
                </span>
                <!-- Raw data Link. ->
                <span class="link-block">
                  <a href="https://iprc-dip.github.io/Chip-Design-LLM-Zoo/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have 
              achieved breakthrough performance on tasks with explicit, automatable verification, such as software 
              programming and mathematical problems. Extending RLVR to electronic design automation (EDA), 
              especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and 
              accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive 
              computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog 
              generation LLMs. First, we develop a rule-based testbench generator that performs robust 
              equivalence checking against golden references. Second, we propose a round-trip data synthesis 
              method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-
              NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a 
              high-quality dataset. Third, we employ a two-stage ``distill-then-RL'' training pipeline: distillation for 
              the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can 
              reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1, 
              achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior 
              state-of-the-art by 12~21 %, while matching or even exceeding the performance of 671B DeepSeek-R1. 
              We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <!-- Overview. -->
      <div class="columns">
        <div class="column">
          <h2 class="title is-3 is-centered has-text-centered">Overview</h2>
          <div class="content">
            <p>
              Our framework comprises one automated testbench generation framework and 5 stages. Stages 1~3 constitute the distillation phase, and stages 4~5 comprise the reinforcement learning phase.
            </p>
          </div>
          <img src="./static/images/overview.png">
            <p>
              <b>1. Code-to-NL</b>: Following prior work [codev, mgverilog], we collect Verilog code snippets from GitHub and use DeepSeek-V3 to produce corresponding natural-language summaries, creating an NL-code corpus with approximately 150K data samples.
            </p>
            <p>
              <b>2. NL-to-Code</b>: Using DeepSeek-R1, we take each NL description from stage 1 and generate the "thought" as well as an Verilog code snippet, producing NL–thought–code triples.
            </p>
            <p>
              <b>3. Difficulty Filtering and Supervised Fine-Tuning</b>: We first filter the dataset by removing any examples for which base LLMs (e.g., Qwen2.5-Coder-Instruct-7B/32B) can generate correct code in any of 5 attempts (correctness is verified using our automatically generated testbench). We then perform SFT on the base LLM to bootstrap their reasoning ability, yielding the distilled model, CodeV-R1-Distill. This stage uses approximately 87K examples.
            </p>
            <p>
              <b>4. Equivalence Checking.</b> We use our automated testbench to verify equivalence between the original snippets and the newly generated snippets. Any non-equivalent pairs are discarded, while equivalent pairs are retained as high-quality data for subsequent RL training. After this filtering, approximately 87K examples remain.
            </p>
            <p>
              <b>5. Difficulty Filtering and Reinforcement Learning.</b> We again filter the retained set by removing any examples where the distilled model CodeV-R1-Distill generates correct code in any of 5 attempts (as checked by the testbench). We then apply our adaptive DAPO algorithm, a novel RLVR algorithm, to further improve Verilog-generation performance, resulting in the final model, CodeV-R1.
            </p>
          </div>
        </div>
      </div>
      <!--/ Overview. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3"> Multi-level Code Summarization</h2>
            <p>
              Manual annotation is prohibitively time-consuming and costly. Hence, we employed GPT-3.5 to generate high-level summaries for each Verilog module as its requirement description. As analyzed in VerilogEval, when required for summarising, LLMs often produce verbose descriptions, preferring line-by-line explanations over high-level summaries. To address this issue, we introduce a multi-level summarization method, employing few-shot learning to guide GPT-3.5 in first producing detailed descriptions and then abstracting high-level summaries.
            </p>
            <p>
              An actual example of the prompt for multi-level summarization. (a) The prompt provided to GPT-3.5. (b) An example of the demonstrations, with code, low-level descriptions, and high-level summaries. (c) Summaries responded from GPT-3.5 with and (d) without multi-level summarization.
            </p>
            <img src="./static/images/multilevel_sum_demo.png">
            </p>
          </div>
        </div>
      </div> -->

      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3"> Main Results </h2>
            <p>
              We compares the main results of our CodeV-R1 with baseline methods on the VerilogEval v1/v2 and RTLLM v1.1/v2.0 benchmarks. 
              Here we present the results of our distilled model and RL model. For baseline comparisons, we manually tested GPT, DeepSeek, and Qwen models, while sourcing results from RTLCoder, BetterV, CodeV, and CraftRTL.
            </p>
            <p>
              Our CodeV-R1 model achieves 68.6% and 72.9% pass@1 accuracy on VerilogEval v2 and RTLLM v1.1, respectively. On RTLLM v1.1, it outperforms previous state-of-the-art models by approximately 20%, and surpasses the 671B DeepSeek-R1 on both RTLLM v1.1 and v2.0.
            </p>
            <p>The following two tables are results on VerilogEval v1 / RTLLM v1.1, and VerilogEval v2 / RTLLM v2.0.</p>
            <img src="./static/images/result_v1.png">
            <p>For VerilogEval v1 and RTLLM v1.1, we evaluate the models with *, while other results are sourced from their papers.</p>
            <img src="./static/images/result_v2.png">
            <p>For VerilogEval v2 and RTLLM v2.0, we evaluate all models in this table. The SR means Specification-to-RTL, while CC means Code Completion.</p>
            </p>
          </div>
        </div>
      </div>


      <!-- <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3"> LLM-generated Verilog code </h2>
            <p>
              We have collected existing LLMs of Verilog code and demonstrated their performance on VerilogEval and RTLLM in
              <a href="https://iprc-dip.github.io/Chip-Design-LLM-Zoo/" target="_blank">Chip Design LLM Zoo</a>.
            </p>
          </div>
        </div>
      </div> -->

      <!-- Method -->

      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Quick Start</h2>
          <pre style="width: 100%; overflow: auto; background: none;">
          <code class="python" style="min-width: 100%; width: 0px; overflow: scroll; font-family: 'Cascadia Code', 'Menlo', 'Courier New', monospace;">
from transformers import pipeline
import torch
prompt= "FILL IN THE QUESTION"
generator = pipeline(
  model="CODEV",
  task="text-generation",
  torch_dtype=torch.bfloat16,
  device_map="auto",
)
result = generator(prompt , max_length=2048,num_return_sequences=1, temperature=0.0)
response = result[0]["generated_text"]
print("Response:", response)
            </code>
          </pre>
        </div>
      </div>
    </div> -->
  
    <!-- <hr/> -->
    <!-- <section class="section" id="BibTeX"> -->
      <!-- <div class="columns is-centered"> -->
        <div class="container content is-max-desktop">
            <h2 class="title is-3"> BibTex </h2>
              <pre>
                <code class="nohighlight" style="background-color: transparent; color: black; font-family: monospace;">
                  [TODO]
                </code>
              </pre>
      </div>
      <!-- </section> -->
  </section>

  <!-- <hr/> -->
  

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Thanks for the website template <a href="https://nerfies.github.io">Nerfies</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
