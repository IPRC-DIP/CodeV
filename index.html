<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization">
  <meta name="keywords" content="Language Language Model, Program synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- highlight code -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a>Yang Zhao</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a>Di Huang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a>Chongxiao Li</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a>Pengwei Jin</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a>Ziyuan Nan</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a>Tianyun Ma</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Lei Qi</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Yansong Pan</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a>Zhenxing Zhang</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a>Rui Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Xishan Zhang</a><sup>1,4</sup>,
              </span>
              <span class="author-block">
                <a>Zidong Du</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Qi Guo</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Xing Hu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Yunji Chen</a><sup>1,2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>SKL of Processors, Institute of Computing Technology,
                CAS</span><br>
              <span class="author-block"><sup>2</sup>University of Chinese Academy of Sciences</span><br>
              <span class="author-block"><sup>3</sup>University of Science and Technology of China</span><br>
              <span class="author-block"><sup>4</sup>Cambricon Technologies</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2407.10424" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/iprc-dip/CodeV" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Raw data Link-->
                <span class="link-block">
                  <a href="https://huggingface.co/yang-z"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-cloud-download-alt"></i>
                    </span>
                    <span>Download</span>
                  </a>
                </span>
                <!-- Raw data Link-->
                <span class="link-block">
                  <a href="https://iprc-dip.github.io/Chip-Design-LLM-Zoo/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The increasing complexity and high costs associated with modern processor design
              have led to a surge in demand for processor design automation. Instruction-tuned
              large language models (LLMs) have demonstrated remarkable performance in
              automatically generating code for general-purpose programming languages like
              Python. However, these methods fail on hardware description languages (HDLs)
              like Verilog due to the scarcity of high-quality instruction tuning data, as even
              advanced LLMs like GPT-3.5 exhibit limited performance on Verilog generation.
              Regarding this issue, we observe that (1) Verilog code collected from the real world
              has higher quality than those generated by LLMs. (2) LLMs like GPT-3.5 excel in
              summarizing Verilog code rather than generating it. Based on these observations,
              this paper introduces CodeV, a series of open-source instruction-tuned Verilog
              generation LLMs. Instead of generating descriptions first and then getting the
              corresponding code from advanced LLMs, we prompt the LLM with Verilog code
              and let the LLM generate the corresponding natural language description by multi-
              level summarization. Experimental results show that CodeV relatively surpasses
              the previous open-source SOTA by 14.4% (BetterV in VerilogEval) and 11.3%
              (RTLCoder in RTLLM) respectively, and also relatively outperforms previous
              commercial SOTA GPT-4 by 22.1% in VerilogEval.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <!-- Overview. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Overview</h2>
          <div class="content">
            <p>
              We first collect and filter high-quality Verilog modules
              from open-source codebases. The modules are then sent to GPT-3.5 to request multi-level summaries.
              Pairing high-level descriptions with corresponding modules, the high-quality dataset is utilized to
              fine-tune base LLMs, yielding CodeV models.
            </p>
          </div>
          <img src="./static/images/overview.png">
        </div>
      </div>
      <!--/ Overview. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3"> Multi-level Code Summarization</h2>
            <p>
              Manual annotation is prohibitively time-consuming and costly. Hence, we employed GPT-3.5 to generate high-level summaries for each Verilog module as its requirement description. As analyzed in VerilogEval, when required for summarising, LLMs often produce verbose descriptions, preferring line-by-line explanations over high-level summaries. To address this issue, we introduce a multi-level summarization method, employing few-shot learning to guide GPT-3.5 in first producing detailed descriptions and then abstracting high-level summaries.
            </p>
            <p>
              An actual example of the prompt for multi-level summarization. (a) The prompt provided to GPT-3.5. (b) An example of the demonstrations, with code, low-level descriptions, and high-level summaries. (c) Summaries responded from GPT-3.5 with and (d) without multi-level summarization.
            </p>
            <img src="./static/images/multilevel_sum_demo.png">
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3"> Main Results </h2>
            <p>
              We compares the main results of our CodeV with baseline methods on the VerilogEval and RTLLM benchmarks. We test CodeLlama, DeepSeek-Coder, and CodeQwen on RTLLM, while other baseline results are sourced from RTLCoder or BetterV paper. For a fair comparison, we also evaluate our models trained on comparable-size datasets against RTLCoder.
            </p>
            <p>
              Comparison of our CodeV models against various baseline models. Some data are missing due to the models being close-sourced and the data not being reported previously. The best results are highlighted in bold. In VerilogEval, CodeV outperforms all previous methods across all metrics, relatively surpassing previous open-source SOTA BetterV by 14.4% and GPT-4 by 22.1% on average. In RTLLM, CodeV relatively surpasses previous open-source SOTA RTLCoder by 11.3%.</p>
            <img src="./static/images/result1.png">
            </p>
          </div>
        </div>
      </div>


      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3"> LLM-generated Verilog code </h2>
            <p>
              We have collected existing LLMs of Verilog code and demonstrated their performance on VerilogEval and RTLLM in
              <a href="https://iprc-dip.github.io/Chip-Design-LLM-Zoo/" target="_blank">Chip Design LLM Zoo</a>.
            </p>
          </div>
        </div>
      </div>

      <!-- Method -->

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Quick Start</h2>
          <pre style="width: 100%; overflow: auto; background: none;">
          <code class="python" style="min-width: 100%; width: 0px; overflow: scroll; font-family: 'Cascadia Code', 'Menlo', 'Courier New', monospace;">
from transformers import pipeline
import torch
prompt= "FILL IN THE QUESTION"
generator = pipeline(
  model="CODEV",
  task="text-generation",
  torch_dtype=torch.bfloat16,
  device_map="auto",
)
result = generator(prompt , max_length=2048,num_return_sequences=1, temperature=0.0)
response = result[0]["generated_text"]
print("Response:", response)
            </code>
          </pre>
        </div>
      </div>
    </div>
  
    <!-- <hr/> -->
    <!-- <section class="section" id="BibTeX"> -->
      <!-- <div class="columns is-centered"> -->
        <div class="container content is-max-desktop">
            <h2 class="title is-3"> BibTex </h2>
              <pre>
  <code class="nohighlight" style="background-color: transparent; color: black; font-family: monospace;">@misc{zhao2024codevempoweringllmsverilog,
    title={CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization},
    author={Yang Zhao and Di Huang and Chongxiao Li and Pengwei Jin and Ziyuan Nan and Tianyun Ma and Lei Qi and Yansong Pan and Zhenxing Zhang and Rui Zhang and Xishan Zhang and Zidong Du and Qi Guo and Xing Hu and Yunji Chen},
    year={2024},
    eprint={2407.10424},
    archivePrefix={arXiv},
    primaryClass={cs.PL},
    url={https://arxiv.org/abs/2407.10424},
  }</code>
              </pre>
      </div>
      <!-- </section> -->
  </section>

  <!-- <hr/> -->
  

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Thanks for the website template <a href="https://nerfies.github.io">Nerfies</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>